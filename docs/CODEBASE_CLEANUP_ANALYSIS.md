# CADS Research Visualization - Codebase Cleanup Analysis

## ğŸš¨ Issues Identified

### 1. **Nested Git Repository**
- `cads/.git/` contains a full git repository pointing to the same remote
- This creates confusion and potential conflicts
- **Status**: âŒ **CRITICAL - NEEDS IMMEDIATE CLEANUP**

### 2. **Data Duplication Across 4+ Locations**
Data is scattered and duplicated across multiple directories:

```
ğŸ“Š DATA DUPLICATION MAP:

1. data/processed/           # âœ… SHOULD BE PRIMARY
   â”œâ”€â”€ cluster_themes.json
   â”œâ”€â”€ clustering_results.json
   â”œâ”€â”€ visualization-data.json
   â””â”€â”€ [compressed versions]

2. cads/data/               # âŒ DUPLICATE + EXTRA FILES
   â”œâ”€â”€ cluster_themes.json      [DUPLICATE]
   â”œâ”€â”€ clustering_results.json  [DUPLICATE]
   â”œâ”€â”€ umap_coordinates.json    [UNIQUE]
   â””â”€â”€ processing_summary.json  [UNIQUE]

3. visuals/data/            # âŒ DUPLICATE
   â”œâ”€â”€ cluster_themes.json      [DUPLICATE]
   â”œâ”€â”€ clustering_results.json  [DUPLICATE]
   â””â”€â”€ umap_coordinates.json    [DUPLICATE]

4. visuals/public/data/     # âœ… PRODUCTION DATA (KEEP)
   â”œâ”€â”€ cluster_themes.json      [PRODUCTION]
   â”œâ”€â”€ clustering_results.json  [PRODUCTION]
   â”œâ”€â”€ visualization-data.json  [PRODUCTION]
   â”œâ”€â”€ search-index.json        [PRODUCTION]
   â””â”€â”€ [compressed versions]    [PRODUCTION]
```

### 3. **Model File Duplication**
```
ğŸ“¦ MODEL DUPLICATION:

1. cads/models/             # âŒ DUPLICATE
   â”œâ”€â”€ hdbscan_model.pkl
   â””â”€â”€ umap_model.pkl

2. visuals/models/          # âŒ DUPLICATE
   â”œâ”€â”€ hdbscan_model.pkl
   â””â”€â”€ umap_model.pkl
```

### 4. **Inconsistent Directory Structure**
- Multiple `tests/` directories (partially cleaned up)
- Inconsistent data organization
- No clear data flow pipeline

## ğŸ¯ Proposed Clean Architecture

### **Single Source of Truth Data Flow**
```
ğŸ“Š CLEAN DATA ARCHITECTURE:

data/                       # ğŸ¯ SINGLE SOURCE OF TRUTH
â”œâ”€â”€ raw/                    # Raw input data
â”œâ”€â”€ processed/              # Pipeline outputs (JSON)
â”œâ”€â”€ models/                 # ML models (PKL files)
â””â”€â”€ search/                 # Search indexes

cads/                       # ğŸ”§ PROCESSING ENGINE ONLY
â”œâ”€â”€ data_loader.py          # Core processing logic
â”œâ”€â”€ process_data.py         # Pipeline orchestration
â”œâ”€â”€ requirements.txt        # Dependencies
â”œâ”€â”€ .env.example           # Config template
â””â”€â”€ [NO DATA DIRECTORIES]  # Data lives in /data

visuals/public/data/        # ğŸŒ PRODUCTION WEB DATA
â”œâ”€â”€ [symlinks to /data/processed/] # OR
â”œâ”€â”€ [automated copy from /data/]   # OR
â””â”€â”€ [build process copies]         # Deployment only
```

## ğŸ§¹ Cleanup Plan

### **Phase 1: Remove Nested Git Repository**
```bash
# IMMEDIATE - Remove nested git repo
rm -rf cads/.git/
```

### **Phase 2: Consolidate Data Directories**
```bash
# Move unique files from cads/data/ to data/processed/
mv cads/data/umap_coordinates.json data/processed/
mv cads/data/processing_summary.json data/processed/

# Remove duplicate data directories
rm -rf cads/data/
rm -rf visuals/data/

# Consolidate models
mv cads/models/* data/models/ 2>/dev/null || true
mv visuals/models/* data/models/ 2>/dev/null || true
rm -rf cads/models/
rm -rf visuals/models/
```

### **Phase 3: Update Code References**
Update all scripts and code to reference the centralized data location:
- `cads/process_data.py` â†’ output to `data/processed/`
- `cads/data_loader.py` â†’ read from `data/processed/`
- Tests â†’ reference `data/` for fixtures

### **Phase 4: Production Data Pipeline**
Create automated pipeline to sync `data/processed/` â†’ `visuals/public/data/`

## ğŸ“‹ Detailed File Analysis

### **Files to Keep (Unique Content)**
```
âœ… UNIQUE FILES TO PRESERVE:
- cads/data/umap_coordinates.json     â†’ move to data/processed/
- cads/data/processing_summary.json  â†’ move to data/processed/
- data/cads.txt                       â†’ keep (input data)
- visuals/public/data/README.md       â†’ keep (documentation)
```

### **Files to Remove (Duplicates)**
```
âŒ DUPLICATE FILES TO REMOVE:
- cads/data/cluster_themes.json
- cads/data/clustering_results.json
- visuals/data/* (entire directory)
- cads/models/* (after moving to data/models/)
- visuals/models/* (after moving to data/models/)
```

### **Production Files (Keep in Place)**
```
ğŸŒ PRODUCTION FILES (KEEP):
- visuals/public/data/* (all files - needed for web deployment)
```

## ğŸ”„ Proposed Data Flow

### **Development Pipeline**
```
1. cads/process_data.py
   â†“ (processes data)
2. data/processed/
   â†“ (build/deploy process)
3. visuals/public/data/
   â†“ (web serving)
4. Production Website
```

### **Benefits of Clean Architecture**
1. **Single Source of Truth**: All data originates from `data/`
2. **Clear Separation**: Processing (`cads/`) vs Storage (`data/`) vs Serving (`visuals/public/data/`)
3. **No Duplication**: Each file exists in one canonical location
4. **Maintainable**: Clear data flow and dependencies
5. **Deployable**: Production data is separate from development data

## âš ï¸ Risks and Considerations

### **Before Cleanup**
1. **Backup Current State**: Create full backup before any changes
2. **Test Dependencies**: Ensure no hardcoded paths break
3. **Verify Production**: Confirm visuals/public/data/ is actually used in production
4. **Check Deployment**: Understand how Vercel deploys the data

### **Potential Breaking Changes**
- Scripts that hardcode paths to `cads/data/` or `visuals/data/`
- Tests that expect data in specific locations
- Deployment processes that rely on current structure

## ğŸš€ Implementation Priority

### **HIGH PRIORITY (Do First)**
1. âœ… Remove nested git repository (`rm -rf cads/.git/`)
2. âœ… Backup current state
3. âœ… Move unique files to centralized location

### **MEDIUM PRIORITY**
4. Update code references to use centralized data
5. Remove duplicate directories
6. Update documentation

### **LOW PRIORITY**
7. Optimize production data pipeline
8. Add automated sync processes
9. Implement data validation

Would you like me to proceed with the cleanup implementation?