# Project Structure & Organization

## Repository Architecture

The CADS Research Visualization System follows a clear separation of concerns with distinct modules for data processing, visualization, database management, and utilities.

## Directory Structure

```
CADS-Research-Visualization/
â”œâ”€â”€ ğŸ“Š cads/                          # Core data processing pipeline
â”œâ”€â”€ ğŸ¨ visuals/                       # Interactive visualization dashboard  
â”œâ”€â”€ ğŸ—„ï¸ database/                      # Database schema and migrations
â”œâ”€â”€ ğŸ”§ scripts/                       # Organized utility scripts
â”œâ”€â”€ ğŸ“š docs/                          # Centralized documentation
â”œâ”€â”€ ğŸ“¦ data/                          # Processed data storage
â””â”€â”€ [config files]                   # .env, vercel.json, etc.
```

## Core Modules

### `cads/` - Data Processing Pipeline
**Purpose**: Core ML pipeline for research data processing
**Pattern**: Modular Python package with clear separation of concerns

```
cads/
â”œâ”€â”€ data_loader.py          # DataProcessor class - database & embeddings
â”œâ”€â”€ process_data.py         # Pipeline orchestration - UMAP & HDBSCAN  
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ .env.example           # Environment template
â”œâ”€â”€ data/                  # Generated data files (JSON)
â”œâ”€â”€ models/                # Trained ML models (PKL files)
â””â”€â”€ tests/                 # Comprehensive test suite
```

**Key Classes**:
- `DataProcessor` - Main data loading and embedding generation
- Pipeline functions for UMAP reduction and HDBSCAN clustering

### `visuals/` - Visualization Dashboard
**Purpose**: Interactive web-based research exploration
**Pattern**: Static web application with modular JavaScript

```
visuals/
â”œâ”€â”€ public/                # Web interface files
â”‚   â”œâ”€â”€ index.html        # Main dashboard interface
â”‚   â”œâ”€â”€ app.js           # Visualization logic & state management
â”‚   â””â”€â”€ data/            # Visualization data files (JSON + gzip)
â”œâ”€â”€ data/                 # Raw visualization data
â”œâ”€â”€ models/              # Visualization ML models
â””â”€â”€ tests/               # Visualization-specific tests
```

**JavaScript Architecture**:
- Single global `app` object for state management
- Modular functions for filtering, search, and visualization
- Event-driven UI updates with DOM manipulation

### `database/` - Schema Management
**Purpose**: PostgreSQL schema definitions and migrations
**Pattern**: SQL-first database design with vector extensions

```
database/
â”œâ”€â”€ schema/              # Table definitions
â”‚   â”œâ”€â”€ create_cads_tables.sql      # Complete CADS schema
â”‚   â””â”€â”€ create_cads_tables_simple.sql
â””â”€â”€ migrations/          # Future database migrations
```

**Schema Design**:
- `cads_researchers` - Faculty profiles with OpenAlex integration
- `cads_works` - Research papers with 384-dimensional embeddings
- `cads_topics` - Hierarchical topic classifications
- Vector indexes for semantic search capabilities

### `scripts/` - Utility Scripts
**Purpose**: Organized automation and maintenance scripts
**Pattern**: Functional categorization with clear working versions

```
scripts/
â”œâ”€â”€ migration/           # Database setup scripts
â”‚   â”œâ”€â”€ execute_cads_migration.py    # âœ… Main working script
â”‚   â””â”€â”€ legacy/                      # Archived attempts
â”œâ”€â”€ processing/          # Data collection and transformation
â”‚   â”œâ”€â”€ process_cads_with_openalex_ids.py
â”‚   â””â”€â”€ migrate_cads_data_to_cads_tables.py
â””â”€â”€ utilities/           # Verification and maintenance
    â””â”€â”€ check_cads_data_location.py
```

**Script Conventions**:
- Working scripts marked with âœ… in documentation
- Legacy/deprecated scripts moved to `legacy/` folders
- Clear naming convention: `verb_object_context.py`

### `data/` - Data Storage
**Purpose**: Centralized data files organized by processing stage
**Pattern**: Clear data flow from raw to processed to search-ready

```
data/
â”œâ”€â”€ raw/                 # Original data files
â”œâ”€â”€ processed/           # ML pipeline outputs
â”‚   â”œâ”€â”€ cluster_themes.json        # AI-generated themes
â”‚   â”œâ”€â”€ clustering_results.json    # HDBSCAN results
â”‚   â””â”€â”€ visualization-data.json    # Complete dataset
â””â”€â”€ search/              # Search indexes
    â””â”€â”€ search-index.json          # Pre-built search index
```

**Data Flow**:
1. Raw data â†’ `data/raw/`
2. ML processing â†’ `data/processed/`
3. Search indexing â†’ `data/search/`
4. Visualization â†’ `visuals/public/data/` (with gzip compression)

## Architectural Patterns

### Separation of Concerns
- **Data Processing**: Pure Python with ML libraries
- **Visualization**: Pure JavaScript with WebGL
- **Database**: PostgreSQL with vector extensions
- **Scripts**: Functional utilities with clear purposes

### Configuration Management
- **Environment Variables**: `.env` files for sensitive configuration
- **JSON Configuration**: `vercel.json` for deployment settings
- **Python Requirements**: `requirements.txt` for dependency management

### Testing Strategy
- **Structure Tests**: Validate repository organization
- **Connection Tests**: Database connectivity verification
- **Pipeline Tests**: End-to-end ML pipeline validation
- **Integration Tests**: Component interaction testing

### Documentation Organization
```
docs/
â”œâ”€â”€ setup/               # Installation and configuration guides
â”œâ”€â”€ pipeline/            # Technical architecture documentation  
â””â”€â”€ migration/           # Historical context and migration records
```

## File Naming Conventions

### Python Files
- **Modules**: `snake_case.py` (e.g., `data_loader.py`)
- **Scripts**: `verb_object_context.py` (e.g., `execute_cads_migration.py`)
- **Tests**: `test_functionality.py` (e.g., `test_basic_structure.py`)

### Data Files
- **JSON**: `descriptive_name.json` (e.g., `clustering_results.json`)
- **Compressed**: `name.json.gz` for web delivery
- **Models**: `algorithm_model.pkl` (e.g., `hdbscan_model.pkl`)

### Documentation
- **README files**: One per major directory explaining purpose and usage
- **Markdown docs**: Descriptive names in `docs/` subdirectories
- **Analysis files**: `CAPS_ANALYSIS.md` for repository-level documentation

## Import Patterns

### Python Imports
```python
# Standard library first
import os, sys, json

# Third-party libraries
import pandas as pd, numpy as np
from sentence_transformers import SentenceTransformer

# Local modules
from data_loader import DataProcessor
```

### JavaScript Modules
- No module system - global `app` object pattern
- Feature-based function organization
- Event-driven architecture with DOM manipulation

## Data Processing Flow

1. **Database Connection** â†’ `DataProcessor` class initialization
2. **Data Loading** â†’ SQL queries to fetch research data
3. **Embedding Generation** â†’ sentence-transformers for semantic vectors
4. **Dimensionality Reduction** â†’ UMAP for 2D coordinates
5. **Clustering** â†’ HDBSCAN for research theme groups
6. **Theme Generation** â†’ AI-powered cluster descriptions
7. **Output Generation** â†’ JSON files for visualization
8. **Compression** â†’ Gzip for web delivery

## Development Workflow

1. **Environment Setup** â†’ Copy `.env.example`, install dependencies
2. **Database Setup** â†’ Run migration scripts
3. **Data Processing** â†’ Execute pipeline scripts in order
4. **Testing** â†’ Run test suite to validate functionality
5. **Local Development** â†’ Start web server for visualization
6. **Deployment** â†’ Automatic via Vercel on Git push